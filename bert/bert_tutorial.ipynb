{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial_link:\n",
    "\n",
    "https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing BERT From Scratch\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT(Bidirectional Encoder Representations from Transformers) is a language represtation model developed by Google. \n",
    "A Language Representation Model refers to a computational model designed to understand, generate, and represent human language in a meaningful way for tasks such as translation, sentiment analysis, or conversation. These model are pretrained on human language data to understand human language firts,then its fine-tuned to do specific task. \n",
    "\n",
    "## How BERT WORK:\n",
    "\n",
    "1. BERT looks a sequence to predict word in both left-to-right and right-to-left. For example a sentence like \"A girl play in ___ with football \". Other model would take only consider either the first part or second part to predict playground for blank but BERT consider both the part.\n",
    "\n",
    "2. Instead of predicting next word,bert randomly mask word in a sentence then takes account full account of sentcence undertand its context to preidict the mask word.\n",
    "\n",
    "3. BERT use encoder part of a transformer to understand the context of a language. Words are first converted into tokens.Tokens are neumerical represtation of a word. Tokens are uesd as a input for encoder.Tokens are converted into vector for neural networks to process.\n",
    "\n",
    "4. BERT needs the input to be massaged and decorated with some extra metadata:\n",
    "\n",
    "**Token Embeddings**: [CLS] token is added to the input word token at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "\n",
    "**Segment Embeddings**: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.\n",
    "\n",
    "**Positional Embeddings**: A positional embedding is added to each token to indicate its position in the sentence.\n",
    "\n",
    "![](2024-09-16-12-16-13.png)\n",
    "\n",
    "**Training** : \n",
    "After embedding data,model is ready to be trained. Bert is traing using this two strategies:\n",
    "\n",
    "4. Masked Language Model (MLM): Bert goes through entire sequence tries to predict Masked word based on the context of non-masked word and traiend along the way.\n",
    "\n",
    "5. Next Sentence Prediction (NSP): Bert takes a pair of sentences and predict if the second sentence comes next of first sentence. \n",
    "\n",
    "\n",
    "## Architecture:\n",
    "BERT-Base: 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters\n",
    "\n",
    "BERT-Large: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/movie_conversations.tsv','r') as tsv_file:\n",
    "    with open('./datasets/movie_conversations.txt','w') as txt_file:\n",
    "        for line in tsv_file:\n",
    "            txt_file.write(line.replace('\\t',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/movie_lines.tsv','r') as tsv_file:\n",
    "    with open('./datasets/movie_lines.txt','w') as txt_file:\n",
    "        for line in tsv_file:\n",
    "            txt_file.write(line.replace('\\t',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import transformers, datasets\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 19\u001b[0;36m\n\u001b[0;31m    ids = eval(con.split(\" +++$+++ \")[-1])\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    u0 u2 m0 ['L194' 'L195' 'L196' 'L197']\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN=64\n",
    "\n",
    "corpus_movie_conv=\"./datasets/movie_conversations.txt\"\n",
    "corpus_movie_lines =\"./datasets/movie_lines.txt\"\n",
    "\n",
    "with open(corpus_movie_conv,'r',encoding='iso-8859-1') as c:\n",
    "    conv=c.readlines()\n",
    "with open(corpus_movie_lines,'r',encoding='iso-8859-1') as l:\n",
    "    lines=l.readlines()\n",
    "\n",
    "lines_dic ={}\n",
    "\n",
    "for line in lines:\n",
    "    objects =line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]]= objects[-1]\n",
    "\n",
    "pairs=[]\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs=[]\n",
    "\n",
    "        if i== len(ids) -1:\n",
    "            break\n",
    "\n",
    "        first=lines_dic[ids[i]].strip()\n",
    "        second=lines_dic[ids[i+1]].strip()\n",
    "\n",
    "        qa_pairs.append(' '.join(first.spit()[:MAX_LEN]))\n",
    "        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordpiece Embeddings:\n",
    " Tokenaization is the process of breaking down text into samller units called \"tokens\",which are then converted into a numerical representation.\n",
    "\n",
    " In bert words are represetned through a vector which is stored in lookuptable.For unknown word it split the word into known subword which embeddings are stored in the lookuptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m text_data\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m file_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#wordPiece Tokenizer\n",
    "os.mkdir('./data')\n",
    "text_data=[]\n",
    "file_count=0\n",
    "\n",
    "for sample in tqdm.tqdm(x[0] for x in pairs):\n",
    "    text_data.append(sample)\n",
    "\n",
    "    if len(text_data) == 10000:\n",
    "        with open(f'./data/text_{file_count}.txt','w',encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data=[]\n",
    "        file_count +=1\n",
    "\n",
    "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n",
    "\n",
    "tokenizer= BertWordPieceTokenizer(\n",
    "    clean_text= True,\n",
    "    handle_chinese_chars = False,\n",
    "    strip_accents =False,\n",
    "    lower_case =True\n",
    ")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size = 30_000,\n",
    "    min_frequency=5,\n",
    "    limit_alphabet =1000,\n",
    "    wordpicese_prefix='##',\n",
    "    special_tokens=['[PAD]','[CLS]','[SEP]','[MASK]','[UNK]']\n",
    ")\n",
    "\n",
    "os.mkdir('./bert-it-1')\n",
    "tokenizer.save_model('./bert-it-1','bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt',local_files_only= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self,data_pair,tokenizer,seq_len=64) :\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len =seq_len\n",
    "        self.corpus_lines =len(data_pair)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "    \n",
    "    def __getitem__(self, item) :\n",
    "        t1,t2,is_next_lebel= self.get_sent(item)\n",
    "\n",
    "        t1_random,t1_lebel=self.random_word(t1)\n",
    "        t2_random,t1_lebel =self.random_word(t2)\n",
    "\n",
    "        t1=[self.tokenizer.vocab['[CLS]']]+t1_random+[self.tokenizer.vocab['[SEP]']]\n",
    "        t2=t2_random+[self.tokenizer.vocab['[PAD]']]\n",
    "        t1_label=[self.tokenizer.vocab['[PAD]']]+t1_label+[self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label= t2_label+[self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        segment_label=([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])\n",
    "\n",
    "        bert_input=(t1+t2)[:self.seq_len]\n",
    "        bert_label=(t1_label+t2_label)[:self.seq_len]\n",
    "        padding=[self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len -len(bert_input))]\n",
    "        bert_input.extend(padding),bert_label.extend(padding),segment_label.extend(padding)\n",
    "\n",
    "        output ={\n",
    "            \"bert_input\": bert_input,\n",
    "            \"bert_label\" :bert_label,\n",
    "            \"segment_label\": segment_label,\n",
    "            \"is_next\": is_next_label\n",
    "        }\n",
    "        return {key:torch.tensor(value) for key,value in output.items()}\n",
    "    \n",
    "    def rendom_word(self,sentence):\n",
    "        tokens=sentence.split()\n",
    "        output_label=[]\n",
    "        output=[]\n",
    "\n",
    "        for i,token in enumerate(tokens):\n",
    "            prob=random.random()\n",
    "\n",
    "            token_id =self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            if porb <0.15:\n",
    "                porb /=0.15\n",
    "\n",
    "                if porb <0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                elif porb <0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "                    for i in range(len(token_id)):\n",
    "                        output_label.append(0)\n",
    "            \n",
    "\n",
    "        output=list(itertools.chain(*[[x] if not isinstance(x,list) else x for x in output]))\n",
    "        output_label=list(itertools.chain(*[[x] if not isinstance(x,list) else x for x in output_label]))\n",
    "        assert len(output) ==len(output_label)\n",
    "        return output,output_label\n",
    "    \n",
    "    def get_sent(self,index):\n",
    "        t1,t2=self.get_corpus_line(index)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            return t1,t2,1\n",
    "        \n",
    "        else:\n",
    "            return t1,self.get_random_line(),0\n",
    "        \n",
    "    def get_corpus_line(self,item):\n",
    "        return self.lines[item][0],self.lines[item][1]\n",
    "    \n",
    "    def get_random_line(self):\n",
    "        return self.lines[random.randrange(len(self.lines))][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(torch.nn.model):\n",
    "    def _init__(self,d_model,max_len=128):\n",
    "        super().__init__()\n",
    "\n",
    "        pe=torch.zeros(max_len,d_model).float()\n",
    "        pe.require_grad=False\n",
    "\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos,i]=math.sin(pos/(10000 **((2*i)/d_model)))\n",
    "                pe[pos,i+1]= math.cos(pos/(10000 **((2*(i+1)))/d_model))\n",
    "\n",
    "            self.pe=pe.unsqueeze(0)\n",
    "\n",
    "        def forward(self,x):\n",
    "            return self.pe\n",
    "        \n",
    "    class BERTEmbedding(torch.nn.Model):\n",
    "        def __init__(self,vocab_size,embed_size,seq_len=64,dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.embed_size=embed_size\n",
    "            self.token= torch.nn.Embedding(vocab_size,embed_size,padding_idx=0)\n",
    "            self.segemnt = torch.nn.Embedding(3,embed_size,padding_idx=0)\n",
    "            self.position=PositionalEmbedding(d_model=embed_size,max_len=seq_len)\n",
    "            self.dropout=torch.nn.Dropout(p=dropout)\n",
    "\n",
    "            def forward(self,sequence,segment_label):\n",
    "                x=self.token(sequence)+self.postion(sequence)+self.segment(segment_label)\n",
    "                return self.dropout(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Architecture:\n",
    "BERT needs on only encoder of transformaer as it's goal is to understand contextual reletionships between words in a text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    def __init__(self,heads,d_model,dropout=0.1):\n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "\n",
    "        assert d_model% heads ==0\n",
    "        self.d_k=d_model//heads\n",
    "        self.heads=heads\n",
    "        self.dropout=torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query= torch.nn.Linear(d_model,d_model)\n",
    "        self.key = torch.nn.Linear(d_model,d_model)\n",
    "        self.value=torch.nn.Linear(d_model,d_model)\n",
    "        self.output_linear=torch.nn.Linear(d_model,d_model)\n",
    "\n",
    "    def forward(self,query,key,value,mask):\n",
    "        query= self.query(query)\n",
    "        key=self.key(key)\n",
    "        value=self.value(value)\n",
    "\n",
    "        query=query.view(query.shape[0],-1,self.heads,self.d_k).permute(0,2,1,3)\n",
    "        key=key.view(key.shape[0],-1,self.heads,self.d_k).permute(0,2,1,3)\n",
    "        value=value.view(value.shape[0],-1,self.heads,self.d_k).permute(0,2,1,3)\n",
    "\n",
    "        scores = torch.matmul(query,key.permute(0,1,3,2))/math.sqrt(query.size(-1))\n",
    "\n",
    "        scores= scores.masked_fill(mask==0,-1e9)\n",
    "\n",
    "        weights =F.softmax(scores,dim=1)\n",
    "        weights=self.dropout(weights)\n",
    "\n",
    "        context=torch.matmul(weights,value)\n",
    "\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0],-1,self.heads*self.d_k)\n",
    "\n",
    "        return self.output_liner(context)\n",
    "    \n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,d_module,middle_dim=2048,dropout=0.1):\n",
    "        super(FeedForward,self).__init__()\n",
    "\n",
    "        self.fc1 =torch.nn.Linear(d_module,d_module)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(middle_dim,d_module)\n",
    "\n",
    "        self.dropout =torch.nn.Dropout(dropout)\n",
    "        self.activation=torch.nn.GELU()\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x\n",
    "\n",
    "class NextSentencePrediction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, 2)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # use only the first token which is the [CLS]\n",
    "        return self.softmax(self.linear(x[:, 0]))\n",
    "\n",
    "class MaskedLanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "class BERTLM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
